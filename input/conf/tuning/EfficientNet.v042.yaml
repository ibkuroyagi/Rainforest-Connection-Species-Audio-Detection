# intrepolate shumamura0 stgategy by efficent net b0, update lr=0.001, fix collater_fc and model(balanced)
# PANNs SED architecture
# BCEWithLogitsLoss + Dializerloss + center
# Random 10sec clip
# Using only tp file
# MixUp
# The base model is EfficientNet_b
###########################################################
#                      GLOBAL SETTING                     #
###########################################################
seed: 1
n_class: 24
n_fold: 5
###########################################################
#                    PREPROCESS SETTING                   #
###########################################################
sr: 48000
fft_size: 2048
hop_size: 512
window: hann
num_mels: 256
fmin: 40
fmax: 24000
###########################################################
#                     NETWORK SETTING                     #
###########################################################
model_type: "EfficientNet_b"
model_params:
  classes_num: 24
  efficient_net_name: efficientnet-b0
  feat_dim: 1280
  is_spec_augmenter: false
  use_dializer: true
  sample_rate: 48000
  window_size: 2048
  hop_size: 512
  mel_bins: 256
  fmin: 40
  fmax: 24000
  require_prep: true
###########################################################
#                    TRAINING SETTING                     #
###########################################################
# augmentation
mixup_alpha: 0.1
augmentation_params:
  PinkNoiseSNR: { always_apply: False, p: 0.8, min_snr: 5.0, max_snr: 50.0 }
  GaussianNoiseSNR: { always_apply: False, p: 0.8, min_snr: 5.0, max_snr: 50.0 }
  VolumeControl: { always_apply: False, p: 0.8, db_limit: 10, mode: fade }
# dataset
train_dataset_mode: tp
allow_cache: true # Whether to allow cache in dataset. If true, it requires cpu memory.
is_normalize: false
use_on_the_fly: false
# batch sampler
# batch_sampler_type: MultiLabelBalancedBatchSampler
# collater_fc
max_frames: 938
l_target: 30
collater_mode: binary
random: false
# dataloader
accum_grads: 1
batch_size: 64 # Batch size.
pin_memory: true # Whether to pin memory in Pytorch DataLoader.
num_workers: 4 # Number of workers in Pytorch DataLoader.
# trainer
use_center_loss: true
use_dializer: true
###########################################################
#                       LOSS SETTING                      #
###########################################################
loss_type: BCEWithLogitsLoss
loss_params:
  reduction: mean

dializer_loss_alpha: 0.5
dializer_loss_type: BCEWithLogitsLoss
dializer_loss_params:
  reduction: mean

center_loss_alpha: 5.0e-5
center_loss_params:
  num_classes: 24
  feat_dim: 1280

tsne_params:
  n_components: 2
  random_state: 0
  perplexity: 30
  n_iter: 1000
###########################################################
#              OPTIMIZER & SCHEDULER SETTING              #
###########################################################
optimizer_type: "Adam"
optimizer_params:
  lr: 0.0016
  weight_decay: 1.0e-4
center_loss_optimizer_params:
  lr: 0.0001
  weight_decay: 1.0e-4
scheduler_type: "StepLR"
scheduler_params:
  step_size: 1000 # scheduler step size.
  gamma: 0.5
###########################################################
#                    INTERVAL SETTING                     #
###########################################################
train_max_steps: 3000 # Number of training steps.
save_interval_steps: 3000 # Interval steps to save checkpoint.
eval_interval_epochs: 5 # Interval epochs to evaluate the network.
###########################################################
#                     OTHER SETTING                       #
###########################################################
n_eval_split: 7
sec: 10