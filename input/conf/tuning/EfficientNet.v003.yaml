# intrepolate shumamura0-san stgategy by efficent net b0.(without mixup)
# PANNs SED architecture
# BCE Loss
# Random 10sec clip
# Using only tp file
# MixUp
# The base model is EfficientNet
###########################################################
#                      GLOBAL SETTING                     #
###########################################################
seed: 1
n_class: 24
n_fold: 5
###########################################################
#                    PREPROCESS SETTING                   #
###########################################################
sr: 48000
fft_size: 4096
hop_size: 1024
window: hann
num_mels: 128
fmin: 50
fmax: 16000
###########################################################
#                     NETWORK SETTING                     #
###########################################################
model_type: "EfficientNet_simple" # pretrained model's type.
model_params:
  classes_num: 24
  efficient_net_name: efficientnet-b0
  feat_dim: 1280
  is_spec_augmenter: false
###########################################################
#                    TRAINING SETTING                     #
###########################################################
# augmentation
wave_mode: False
# mixup_alpha: 0.2
# dataset
train_dataset_mode: tp
allow_cache: True # Whether to allow cache in dataset. If true, it requires cpu memory.
is_normalize: False
# collater_fc
max_frames: 512
l_target: 16
collater_mode: binary
random: False
# dataloader
accum_grads: 1
batch_size: 64 # Batch size.
pin_memory: False # Whether to pin memory in Pytorch DataLoader.
num_workers: 2 # Number of workers in Pytorch DataLoader.
# trainer
use_center_loss: False
###########################################################
#                       LOSS SETTING                      #
###########################################################
loss_type: BCEWithLogitsLoss
loss_params:
  reduction: mean

###########################################################
#              OPTIMIZER & SCHEDULER SETTING              #
###########################################################
optimizer_type: "Adam"
optimizer_params:
  lr: 0.0002
scheduler_type: "CosineAnnealingLR"
scheduler_params:
  T_max: 500 # scheduler step size.
  eta_min: 1.0e-7
###########################################################
#                    INTERVAL SETTING                     #
###########################################################
train_max_steps: 3000 # Number of training steps.
save_interval_steps: 1000 # Interval steps to save checkpoint.
eval_interval_epochs: 5 # Interval epochs to evaluate the network.
###########################################################
#                     OTHER SETTING                       #
###########################################################
n_eval_split: 6
