{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36864bitvenvvirtualenv6514465dac1e4c1981844926ede66322",
   "display_name": "Python 3.6.8 64-bit ('venv': virtualenv)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import yaml\n",
    "import torch.nn.functional as F\n",
    "from torchlibrosa.augmentation import SpecAugmentation\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "sys.path.append(\"../input/modules\")\n",
    "from datasets import RainForestDataset\n",
    "from utils import read_hdf5\n",
    "from utils import down_sampler\n",
    "from utils import get_concat_down_frame\n",
    "from utils import get_down_sample_matrix\n",
    "from models import Cnn14_DecisionLevelAtt\n",
    "from models import AttBlock\n",
    "from models import ResNext50\n",
    "from losses import FrameClipLoss\n",
    "from datasets import FeatTrainCollater\n",
    "from datasets import FeatEvalCollater\n",
    "from utils import lwlrap\n",
    "sys.path.append(\"../input/iterative-stratification-master\")\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tp = pd.read_csv('../input/rfcx-species-audio-detection/train_tp.csv')\n",
    "train_fp = pd.read_csv('../input/rfcx-species-audio-detection/train_fp.csv')\n",
    "submission = pd.read_csv('../input/rfcx-species-audio-detection/sample_submission.csv')\n",
    "tmp = pd.DataFrame(train_tp[\"recording_id\"].value_counts()).reset_index().rename(columns={'index': 'recording_id', \"recording_id\": \"count\"})\n",
    "train_tp[\"t_diff\"] = train_tp[\"t_max\"] - train_tp[\"t_min\"]\n",
    "train_tp[\"f_diff\"] = train_tp[\"f_max\"] - train_tp[\"f_min\"]\n",
    "train_fp[\"t_diff\"] = train_fp[\"t_max\"] - train_fp[\"t_min\"]\n",
    "train_fp[\"f_diff\"] = train_fp[\"f_max\"] - train_fp[\"f_min\"]\n",
    "train_tp = pd.merge(train_tp, tmp, on=\"recording_id\", how='inner')\n",
    "train_tp[\"count\"].value_counts()\n",
    "tp_list = train_tp[\"recording_id\"].unique()\n",
    "fp_list = train_fp[\"recording_id\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1     2028\n",
       "2     1716\n",
       "3     1635\n",
       "4     1356\n",
       "5      575\n",
       "6      294\n",
       "7      126\n",
       "8       32\n",
       "10      10\n",
       "9        9\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "tmp = pd.DataFrame(train_fp[\"recording_id\"].value_counts()).reset_index().rename(columns={'index': 'recording_id', \"recording_id\": \"count\"})\n",
    "train_fp = pd.merge(train_fp, tmp, on=\"recording_id\", how='inner')\n",
    "train_fp[\"count\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_path_list = os.listdir(\"../input/rfcx-species-audio-detection/train\")\n",
    "test_path_list = os.listdir(\"../input/rfcx-species-audio-detection/test\")\n",
    "print(f\"train:{len(train_path_list)}, test:{len(test_path_list)}\")\n",
    "# print(\"train\")\n",
    "# for path in tqdm(train_path_list):\n",
    "#     path = os.path.join(\"../input/rfcx-species-audio-detection/train\", path)\n",
    "#     y, sr = librosa.load(path=path, sr=None)\n",
    "#     if (sr != 48000) or (len(y) != 2880000):\n",
    "#         print(path, y.shape, sr)\n",
    "# print(\"test\")\n",
    "# for path in tqdm(test_path_list):\n",
    "#     path = os.path.join(\"../input/rfcx-species-audio-detection/test\", path)\n",
    "#     y, sr = librosa.load(path=path, sr=None)\n",
    "#     if (sr != 48000) or (len(y) != 2880000):\n",
    "#         print(path, y.shape, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(y)\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.imshow(logmel.T, aspect=\"auto\")\n",
    "\n",
    "plt.colorbar()\n",
    "y, sr = librosa.load(path=train_tp_list[10], sr=48000)\n",
    "# y = (y -y.mean()) / y.std()\n",
    "logmel = logmelfilterbank(\n",
    "    y,\n",
    "    sr,\n",
    "    fft_size=2048,\n",
    "    hop_size=512,\n",
    "    win_length=None,\n",
    "    window=\"hann\",\n",
    "    num_mels=128,\n",
    "    fmin=50,\n",
    "    fmax=16000,\n",
    "    eps=1e-10,\n",
    ")\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(y)\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.imshow(logmel.T, aspect=\"auto\")\n",
    "plt.colorbar()\n",
    "\n",
    "# plt.savefig(\"spec.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"dump/wave/train/551385b05.h5\" # \"dump/wave/train/c12e0a62b.h5\"\n",
    "wave = read_hdf5(hdf5_name=path, hdf5_path=\"wave\")\n",
    "mel = read_hdf5(hdf5_name=path, hdf5_path=\"feats\")\n",
    "matrix_tp = read_hdf5(hdf5_name=path, hdf5_path=\"matrix_tp\")\n",
    "matrix_fp = read_hdf5(hdf5_name=path, hdf5_path=\"matrix_fp\")\n",
    "\n",
    "# plt.figure(figsize=(16, 8))\n",
    "# plt.subplot(2, 2, 1)\n",
    "# plt.plot(wave)\n",
    "# plt.subplot(2, 2, 2)\n",
    "# plt.imshow(mel.T, aspect=\"auto\")\n",
    "# plt.colorbar()\n",
    "# plt.subplot(2, 2, 3)\n",
    "# plt.imshow(matrix_tp.T, aspect=\"auto\")\n",
    "# plt.subplot(2, 2, 4)\n",
    "# plt.imshow(matrix_fp.T, aspect=\"auto\")\n",
    "# plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_frame = torch.stack([torch.tensor(down_sampler(matrix_tp[3000:3512])), torch.tensor(down_sampler(matrix_tp[4500:5012]))], dim=0)\n",
    "y_clip = torch.tensor((y_frame>0).any(axis=1), dtype=torch.float)\n",
    "x_batch = torch.stack([torch.tensor(mel[3000:3512]), torch.tensor(mel[4500:5012])], dim=0).transpose(2,1)\n",
    "print(y_frame.shape, y_clip.shape)\n",
    "model = Cnn14_DecisionLevelAtt(sample_rate=16000,\n",
    "        window_size=1024,\n",
    "        hop_size=256,\n",
    "        mel_bins=64,\n",
    "        fmin=50,\n",
    "        fmax=8000,\n",
    "        classes_num=24,\n",
    "        training=False,\n",
    "        require_prep=False,\n",
    "        is_spec_augmenter=False,\n",
    "        mixup_lambda=None,)\n",
    "model.bn0 = nn.BatchNorm2d(128)\n",
    "y = model(x_batch)\n",
    "bce = nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
    "print(y[\"y_frame\"].shape, y[\"y_clip\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = FrameClipLoss()\n",
    "loss = criterion(y[\"y_frame\"], y_frame, y[\"y_clip\"], y_clip)\n",
    "print(loss.item())\n",
    "loss.backward(retain_graph=True)\n",
    "# clip_loss = bce(y[\"y_clip\"], y_clip)\n",
    "# frame_loss = bce(y[\"y_frame\"], y_frame)\n",
    "# print(clip_loss.item())\n",
    "# print(frame_loss.item())\n",
    "# clip_loss.backward(retain_graph=True)\n",
    "# frame_loss.backward(retain_graph=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sumpling function\n",
    "source = y_frame[0]\n",
    "new_source = torch.zeros_like(y[\"y_frame\"][0])\n",
    "l_source = 550\n",
    "l_target = 16\n",
    "l_effect = l_source // l_target\n",
    "for i in range(l_target):\n",
    "    t_start = i * l_effect\n",
    "    t_end = (i+1) * l_effect\n",
    "    if i == l_target -1:\n",
    "        t_end = l_source\n",
    "    new_source[i] = source[t_start:t_end].sum(axis=0) / l_effect\n",
    "    new_source[i] = torch.tensor((source[t_start:t_end]>0).any(axis=0), dtype=torch.float)\n",
    "plt.imshow(new_source.T, aspect=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "columns = [\"recording_id\"] + [f\"s{i}\" for i in range(24)]\n",
    "ground_truth = pd.DataFrame(np.zeros((len(tp_list), 25)), columns=columns)\n",
    "ground_truth[\"recording_id\"] = train_tp[\"recording_id\"].unique()\n",
    "for i, recording_id in enumerate(train_tp[\"recording_id\"].values):\n",
    "    ground_truth.iloc[ground_truth[\"recording_id\"]==recording_id, train_tp.loc[i, \"species_id\"]+1] = 1.0\n",
    "\n",
    "kfold = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "y = ground_truth.iloc[:, 1:].values\n",
    "for i, (train_idx, valid_idx) in enumerate(kfold.split(y, y)):\n",
    "    train_y = ground_truth.iloc[train_idx]\n",
    "    valid_y = ground_truth.iloc[valid_idx]\n",
    "    if i == 0:\n",
    "        break\n",
    "train_dataset = RainForestDataset(\n",
    "    root_dir=\"dump/wave/train\",\n",
    "    train_tp=train_tp,\n",
    "    train_fp=None,\n",
    "    keys=[\"feats\"],\n",
    "    mode=\"tp\",\n",
    "    is_normalize=False,\n",
    "    allow_cache=False,  # keep compatibility\n",
    "    seed=None,\n",
    ")\n",
    "eval_dataset = RainForestDataset(\n",
    "    files=[\n",
    "        os.path.join(\"dump/wave/train\", f\"{recording_id}.h5\")\n",
    "        for recording_id in tp_list[valid_idx]\n",
    "    ],\n",
    "    keys=[\"feats\", \"matrix_tp\"],\n",
    "    mode=\"test\",\n",
    "    is_normalize=False,\n",
    "    allow_cache=False,  # keep compatibility\n",
    "    seed=None,\n",
    "        )\n",
    "\n",
    "train_collater = FeatTrainCollater(\n",
    "    max_frames=512,\n",
    "    l_target=16,\n",
    "    mode=\"binary\",\n",
    ")\n",
    "eval_collater = FeatEvalCollater(\n",
    "    max_frames=512,\n",
    "    n_split=3,\n",
    "    is_label=True,\n",
    ")\n",
    "data_loader = {\n",
    "\"train\": DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=64,\n",
    "        collate_fn=train_collater,\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        pin_memory=False,\n",
    "    ),\n",
    "\"eval\": DataLoader(\n",
    "        eval_dataset,\n",
    "        batch_size=64,\n",
    "        shuffle=False,\n",
    "        collate_fn=eval_collater,\n",
    "        num_workers=2,\n",
    "        pin_memory=False,\n",
    "    ),\n",
    "}\n",
    "clip_list = []\n",
    "for i, batch in enumerate(data_loader[\"train\"]):\n",
    "    x = batch[\"X\"]\n",
    "    y_frame = batch[\"y_frame\"]\n",
    "    y_clip = batch[\"y_clip\"]\n",
    "    clip_list.append(y_clip)\n",
    "    print(x.shape)\n",
    "    print(y_frame.shape)\n",
    "    print(y_clip.shape)\n",
    "    if i == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = []\n",
    "y1_clip = []\n",
    "beginnings1 = []\n",
    "for i, batch in enumerate(data_loader[\"train\"]):\n",
    "    x1.append(batch[\"X\"])\n",
    "    y1_clip.append(batch[\"y_clip\"])\n",
    "    beginnings1.append(batch[\"beginnings\"])\n",
    "    if i == 0:\n",
    "        break\n",
    "x2 = []\n",
    "y2_clip = []\n",
    "beginnings2 = []\n",
    "for i, batch in enumerate(data_loader[\"train\"]):\n",
    "    x2.append(batch[\"X\"])\n",
    "    y2_clip.append(batch[\"y_clip\"])\n",
    "    beginnings2.append(batch[\"beginnings\"])\n",
    "    if i == 0:\n",
    "        break\n",
    "xx1 = torch.cat(x1, dim=0)\n",
    "xx2 = torch.cat(x2, dim=0)\n",
    "yy1_clip = torch.cat(y1_clip, dim=0)\n",
    "yy2_clip = torch.cat(y2_clip, dim=0)\n",
    "ans = xx1 - xx2\n",
    "print(ans.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = []\n",
    "y1_clip = []\n",
    "for i, batch in enumerate(data_loader[\"train\"]):\n",
    "    x1.append(batch[\"X\"])\n",
    "    y1_clip.append(batch[\"y_clip\"])\n",
    "x2 = []\n",
    "y2_clip = []\n",
    "for i, batch in enumerate(data_loader[\"train\"]):\n",
    "    x2.append(batch[\"X\"])\n",
    "    y2_clip.append(batch[\"y_clip\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx1 = torch.cat(x1, dim=0)\n",
    "xx2 = torch.cat(x2, dim=0)\n",
    "yy1_clip = torch.cat(y1_clip, dim=0)\n",
    "yy2_clip = torch.cat(y2_clip, dim=0)\n",
    "ans = xx1 - xx2\n",
    "print(ans.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"valid\"\n",
    "n_eval_split = 3\n",
    "device = \"cpu\"\n",
    "n_class = 24\n",
    "l_spec = 16\n",
    "keys_list = [f\"X{i}\" for i in range(n_eval_split)]\n",
    "y_clip = [\n",
    "    torch.empty((0, n_class)).to(device)\n",
    "    for _ in range(n_eval_split)\n",
    "]\n",
    "y_frame = [\n",
    "    torch.empty((0, l_spec, n_class)).to(device)\n",
    "    for _ in range(n_eval_split)\n",
    "]\n",
    "y_clip_true = torch.empty((0, n_class))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in data_loader[\"eval\"]:\n",
    "        if mode == \"valid\":\n",
    "            y_clip_true = torch.cat([y_clip_true, batch[\"y_clip\"]], dim=0)\n",
    "        x_batchs = [batch[key].to(device) for key in keys_list]\n",
    "        for i in range(n_eval_split):\n",
    "            y_batch_ = model(x_batchs[i])\n",
    "            y_clip[i] = torch.cat([y_clip[i], y_batch_[\"y_clip\"]], dim=0)\n",
    "            y_frame[i] = torch.cat([y_frame[i], y_batch_[\"y_frame\"]], dim=0)\n",
    "# (B, n_eval_split, n_class)\n",
    "y_clip = torch.stack(y_clip, dim=0).detach().cpu().numpy()\n",
    "# (B, n_eval_split, T, n_class)\n",
    "y_frame = torch.stack(y_frame, dim=0).detach().cpu().numpy()\n",
    "if mode == \"valid\":\n",
    "    y_clip_true = y_clip_true.numpy()\n",
    "    score = lwlrap(y_clip_true, y_clip.max(axis=0))\n",
    "    print(f\"score:{score:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = pd.read_csv(\"../input/rfcx-species-audio-detection/ground_truth.csv\")\n",
    "tag = \"v002-cos25\"\n",
    "type = \"wave\"\n",
    "with open(f\"conf/tuning/Cnn14_DecisionLevelAtt.{tag}.yaml\") as f:\n",
    "    config = yaml.load(f, Loader=yaml.Loader)\n",
    "oof_path = f\"exp/{type}/Cnn14_DecisionLevelAtt/{tag}/best_score/oof.h5\"\n",
    "y_clip = read_hdf5(oof_path, \"y_clip\")\n",
    "y_frame = read_hdf5(oof_path, \"y_frame\")\n",
    "print(y_clip.shape, y_frame.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 19\n",
    "recording_id = ground_truth.loc[idx, \"recording_id\"]\n",
    "print(recording_id)\n",
    "dump_path = f\"dump/{type}/train/{recording_id}.h5\"\n",
    "idx_matrix = read_hdf5(dump_path, \"matrix_tp\")\n",
    "l_original = len(idx_matrix)\n",
    "ground_truth_frame = get_down_sample_matrix(idx_matrix, l_target=config[\"l_target\"], max_frames=config[\"max_frames\"], n_eval_split=config[\"n_eval_split\"], mode=\"binary\")\n",
    "concat_y_frame = get_concat_down_frame(y_frame[idx], l_original=l_original, max_frames=config[\"max_frames\"])\n",
    "concat_ground_truth = get_concat_down_frame(ground_truth_frame, l_original=l_original, max_frames=config[\"max_frames\"])\n",
    "\n",
    "true_clip = idx_matrix.any(axis=0).astype(np.int64)[:24].reshape(1, -1)\n",
    "pred_clip = y_clip[idx].max(axis=0)[:24].reshape(1, -1)\n",
    "pred_frame_clip = concat_y_frame.max(axis=0)[:24].reshape(1, -1)\n",
    "score1 = lwlrap(true_clip, pred_clip)\n",
    "score2 = lwlrap(true_clip, pred_frame_clip)\n",
    "score3 = lwlrap(true_clip, pred_clip+pred_frame_clip)\n",
    "\n",
    "print(f\"clip:{score1:.5f}, frame: {score2:.5f}, ensemble: {score3:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(4, 1, 1)\n",
    "plt.title(f\"clip:{score1:.5f}, frame: {score2:.5f}, ensemble: {score3:.5f}\")\n",
    "plt.imshow(idx_matrix.T, aspect=\"auto\")\n",
    "plt.subplot(4, 1, 2)\n",
    "plt.title(\"Clip\")\n",
    "plt.imshow(y_clip[idx].T, aspect=\"auto\")\n",
    "plt.subplot(4, 1, 3)\n",
    "plt.title(\"Frame\")\n",
    "plt.imshow(concat_y_frame.T, aspect=\"auto\")\n",
    "plt.subplot(4, 1, 4)\n",
    "plt.title(\"Frame-true\")\n",
    "plt.imshow(concat_ground_truth.T, aspect=\"auto\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for center loss\n",
    "a = torch.where(torch.tensor([[0,1,0],[1,1,0]])==1)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pred_frame_clip.flatten(), alpha=0.5)\n",
    "plt.plot(true_clip.flatten(), alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(concat_y_frame[:,24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnext50_32x4d = models.resnext50_32x4d(pretrained=True)\n",
    "x1 = torch.stack([x,x,x]).transpose(1,0)\n",
    "conv1 = nn.Conv2d(1, 3, 1, 1)\n",
    "conv_out = conv1(x.unsqueeze(1))\n",
    "conv_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for name,_ in model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = resnext50_32x4d.conv1(x1)\n",
    "print(x.shape)\n",
    "x = resnext50_32x4d.bn1(x)\n",
    "print(x.shape)\n",
    "x = resnext50_32x4d.relu(x)\n",
    "print(x.shape)\n",
    "x = resnext50_32x4d.maxpool(x)\n",
    "print(x.shape)\n",
    "x = resnext50_32x4d.layer1(x)\n",
    "print(x.shape)\n",
    "x = resnext50_32x4d.layer2(x)\n",
    "print(x.shape)\n",
    "x = resnext50_32x4d.layer3(x)\n",
    "print(x.shape)\n",
    "x = resnext50_32x4d.layer4(x)\n",
    "print(x.shape)\n",
    "x = resnext50_32x4d.avgpool(x)\n",
    "print(x.shape)\n",
    "x = torch.flatten(x, 1)\n",
    "print(x.shape)\n",
    "y1 = resnext50_32x4d.fc(x)\n",
    "print(y1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNext50(num_mels=128,\n",
    "        classes_num=25,\n",
    "        training=False,\n",
    "        is_spec_augmenter=True)\n",
    "yy = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(yy[\"y_frame\"].shape, yy[\"y_clip\"].shape, yy[\"embedding\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioTransform:\n",
    "    def __init__(self, always_apply=False, p=0.5):\n",
    "        self.always_apply = always_apply\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, y: np.ndarray):\n",
    "        if self.always_apply:\n",
    "            return self.apply(y)\n",
    "        else:\n",
    "            if np.random.rand() < self.p:\n",
    "                return self.apply(y)\n",
    "            else:\n",
    "                return y\n",
    "\n",
    "    def apply(self, y: np.ndarray):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class Compose:\n",
    "    def __init__(self, transforms: list):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, y: np.ndarray):\n",
    "        for trns in self.transforms:\n",
    "            y = trns(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class OneOf:\n",
    "    def __init__(self, transforms: list):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, y: np.ndarray):\n",
    "        n_trns = len(self.transforms)\n",
    "        trns_idx = np.random.choice(n_trns)\n",
    "        trns = self.transforms[trns_idx]\n",
    "        return trns(y)\n",
    "    \n",
    "class TimeStretch(AudioTransform):\n",
    "    def __init__(self, always_apply=False, p=0.5, max_rate=1.2):\n",
    "        super().__init__(always_apply, p)\n",
    "\n",
    "        self.max_rate = max_rate\n",
    "\n",
    "    def apply(self, y: np.ndarray, **params):\n",
    "        rate = np.random.uniform(0, self.max_rate)\n",
    "        rate = 0.9\n",
    "        augmented = librosa.effects.time_stretch(y, rate)\n",
    "        return augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"dump/wave/train/551385b05.h5\" # \"dump/wave/train/c12e0a62b.h5\"\n",
    "wave = read_hdf5(hdf5_name=path, hdf5_path=\"wave\")\n",
    "transform = TimeStretch(always_apply=True, max_rate=1.1)\n",
    "y_time_stretch = transform(wave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(2880000,) (3200000,)\n"
     ]
    }
   ],
   "source": [
    "print(wave.shape, y_time_stretch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}